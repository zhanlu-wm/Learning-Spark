# Chapter_6 Spark编程进阶

        累加器
        广播变量
        基于分区进行操作
        与外部程序间的管道
        数值RDD的操作

## 1. 累加器

    累加器：提供了将工作节点中的值聚合到驱动器程序中的简单语法。累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。
        创建语法：
            val accumu = sc.accumulator(0)
        获取累加器的值：
            accumu.value
        之后在Spark的转换或行动算子的函数中使用该变量即可。
        需注意：
            1. 由于转换操作是惰性的，在转换操作中对累加器做的修改，只有在之后最近一次的行动操作执行后，才真正发生。行动操作中对累加器的修改，在该操作执行后就会生效。
            2. 工作节点上的任务不能访问累加器的值，对这些任务来说，累加器是一个只写变量，这种模式下累加器的实现可以更加高效。
            3. 转换操作中巧用累加器，可以在我们需要对数据集中的某些数据进行聚合统计时，不必使用filter配合reduce的方式实现。

#### 1. 累加器和容错性

    对于要在行动操作中使用的累加器，Spark只会把每个任务对各累加器的修改应用一次。
    对于要在转换操作中使用的累加器，则可能发生不止一次更新。
    在Spark-Shell中测试情况如下：
        1. 行动操作中的累加器，如果在多个行动操作中（或者Spark-Shell中同一行动操作多次执行）都对该累加器进行了更新（中间没有重置累加器值），那么所有的更新是累加的。
        2. 转换操作中的累加器，如果经该转换操作生成的RDD没有被缓存，那么后续依赖该转换操作的结果RDD的行动操作，每执行一个(或一次)，累加器的值就会（在上一轮更新的基础上）进行新一轮更新。如果经该转换操作生成的RDD被缓存了，只要该缓存一直可用，那么后续依赖该转换操作的结果RDD的行动操作不论执行了多少个(次)，都不会再触发对该累加器的更新，但是如果该缓存失效了或者被清除了，后续再次使用该结果RDD时，由于需要按照该RDD的依赖谱系重新计算它，就会重新触发相应转换操作中的累加器更新。

#### 2. 自定义累加器

    Spark直接支持的累加器有Int、Long、Float、Double类型。
    Spark也允许我们根据相关规范自定义累加器，自定义累加器需要扩展AccumulatorParam。

## 2. 广播变量

    广播变量：可以让程序高效的向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。
    为什么要引入广播变量：Spark会自动把闭包中所有引用到的变量进行序列化后发送到各个工作节点上，这种在各工作节点间共享数据的方式很方便，但也很低效，原因有二：首先这种默认的任务发射机制是专门为小任务进行优化的；其次事实上我们可能会在多个并行操作中使用到同一个变量，但这种方式下，Spark会为每个操作分别发送一遍。如果要发送的数据量比较小，程序是可以良好运行的，但如果发送的数据量较大，从主节点为每个任务(task)都发送一个这样的数据集，其代价将是巨大的。而且，如果后续的操作还要再用到这组数据，Spark就要再向各个任务节点发送一遍。我们可以通过把这组数据变为广播变量来解决这个问题。广播变量其实就是类型为spark.broadcast.Broadcast[T]的一个对象，其中存放着类型为 T的值。可以在任务中通过对Broadcast对象调用value来获取该对象的值。这个值只会被发送到各节点一次，使用的是一种高效的类似BitTorrent的通信机制。

    使用广播变量的过程很简单：
        (1) 通过对一个类型 T 的对象调用SparkContext.broadcast创建出一个Broadcast[T]对象。任何可序列化的类型都可以这么实现。
        (2) 通过 value 属性访问该对象的值（在 Java 中为 value() 方法）。
        (3) 变量只会被发到各个节点一次，应作为只读值处理（修改这个值不会影响到别的节点）。

    广播的优化：
    当广播一个比较大的值时，选择既快又好的序列化格式是很重要的，因为如果序列化对象的时间很长或者传送花费的时间太久，这段时间很容易就成为性能瓶颈。Spark的Scala和Java API中默认使用的序列化库为Java序列化库，因此它对于除基本类型的数组以外的任何对象都比较低效。可以使用spark.serializer属性选择另一个序列化库来优化序列化过程。

## 3. 基于分区进行操作

    基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的工作。Spark提供基于分区的 map 和 foreach，让你的部分代码只对RDD的每个分区运行一次，这样可以帮助降低这些操作的代价。

    按分区执行的操作如下：
    -----------------------------------------------------------------------------------------------------------------------------
    函数名                       调用所提供的                         返回的                     对于RDD[T]的函数签名
    -----------------------------------------------------------------------------------------------------------------------------
    mapPartitions()             该分区中元素的迭代器                  返回的元素的迭代器           f: (Iterator[T]) → Iterator[U]
    mapPartitionsWithIndex()    分区序号，及每个分区中的元素的迭代器     返回的元素的迭代器           f: (Int, Iterator[T]) → Iterator[U]
    foreachPartitions()         元素迭代器                           无                        f: (Iterator[T]) → Unit
    -----------------------------------------------------------------------------------------------------------------------------

## 3. 与外部程序间的管道

    有三种可用的语言供你选择，这可能已经满足了你用来编写 Spark 应用的几乎所有需求。但是，如果 Scala、 Java 以及 Python 都不能实现你需要的功能，那么 Spark也为这种情况提供了一种通用机制，可以将数据通过管道传给用其他语言编写的程序，比如 R 语言脚本。
    
    Spark 在 RDD 上提供 pipe() 方法。 Spark 的 pipe() 方法可以让我们使用任意一种语言实现 Spark 作业中的部分逻辑，只要它能读写 Unix 标准流就行。通过 pipe()，你可以将 RDD 中的各元素从标准输入流中以字符串形式读出，并对这些元素执行任何你需要的操作，然后把结果以字符串的形式写入标准输出——这个过程就是 RDD 的转化操作过程。这种接口和编程模型有较大的局限性， 但是有时候这恰恰是你想要的，比如在 map 或 filter 操作中使用某些语言原生的函数。

## 4. 数值RDD的操作

    Spark对仅包含数值数据的RDD提供了一些描述性的统计操作。Spark的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用stats()时通过一次遍历数据计算出来，并以StatsCounter对象返回。

    StatsCounter中可用的汇总统计数据：
        方法                    含义
        count()                 RDD中的元素个数
        mean()                  元素的平均值
        sum()                   总和
        max()                   最大值
        min()                   最小值
        variance()              元素的方差
        sampleVariance()        从采样中计算出的方差
        stdev()                 标准差
        sampleStdev()           采样的标准差

    如果只想计算这些统计数据中的一个，也可以直接对RDD调用对应的方法，比如rdd.mean()或者rdd.sum()。
